{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Day 4 Text Generation.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVPMJ1IW-_-M",
        "colab_type": "code",
        "outputId": "a18b6f4b-f68b-4302-cf48-0aef8ae219ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from numpy import array\n",
        "from pickle import dump\n",
        "from keras.utils import to_categorical\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2xD0P5Z_cHw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "8ecf0d46-e930-4f20-fa06-2f900a788eb8"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dftMaqJP-_-M",
        "colab_type": "code",
        "outputId": "4d81861a-d880-4f08-b924-af25206166ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 274
        }
      },
      "source": [
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "    # open the file as read only\n",
        "    file = open(filename, 'r')\n",
        "    # read all text\n",
        "    text = file.read()\n",
        "    # close the file\n",
        "    file.close()\n",
        "    return text\n",
        "\n",
        "# load text\n",
        "raw_text = load_doc('/content/drive/My Drive/shakespeare.txt')\n",
        "print(raw_text)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "From fairest creatures we desire increase,\n",
            "That thereby beauty's rose might never die,\n",
            "But as the riper should by time decease,\n",
            "His tender heir might bear his memory:\n",
            "But thou contracted to thine own bright eyes,\n",
            "Feed'st thy light's flame with self-substantial fuel,\n",
            "Making a famine where abundance lies,\n",
            "Thy self thy foe, to thy sweet self too cruel:\n",
            "Thou that art now the world's fresh ornament,\n",
            "And only herald to the gaudy spring,\n",
            "Within thine own bud buriest thy content,\n",
            "And tender churl mak'st waste in niggarding:\n",
            "Pity the world, or else this glutton be,\n",
            "To eat the world's due, by the grave and thee.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qhiu7-g-_-M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "e464514c-0f96-4dad-e14c-d8437aa4c327"
      },
      "source": [
        "# clean\n",
        "tokens = raw_text.split()\n",
        "raw_text = ' '.join(tokens)\n",
        "print(raw_text)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "From fairest creatures we desire increase, That thereby beauty's rose might never die, But as the riper should by time decease, His tender heir might bear his memory: But thou contracted to thine own bright eyes, Feed'st thy light's flame with self-substantial fuel, Making a famine where abundance lies, Thy self thy foe, to thy sweet self too cruel: Thou that art now the world's fresh ornament, And only herald to the gaudy spring, Within thine own bud buriest thy content, And tender churl mak'st waste in niggarding: Pity the world, or else this glutton be, To eat the world's due, by the grave and thee.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HkHgfHj2-_-M",
        "colab_type": "code",
        "outputId": "fa20613a-04f9-4d93-fa59-eda452024709",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 10982
        }
      },
      "source": [
        "#organize into sequences of characters\n",
        "length = 12\n",
        "sequences = list()\n",
        "for i in range(length, len(raw_text)):\n",
        "    # select sequence of tokens\n",
        "    seq = raw_text[i-length:i+1]\n",
        "    # store\n",
        "    sequences.append(seq)\n",
        "    print('Total Sequences: %d' % len(sequences))\n",
        "\n",
        "#Total Sequences: 597\n",
        "\n",
        "# save tokens to file, one dialog per line\n",
        "def save_doc(lines, filename):\n",
        "    data = '\\n'.join(lines)\n",
        "    file = open(filename, 'w')\n",
        "    file.write(data)\n",
        "    file.close()    \n",
        "# save sequences to file\n",
        "out_filename = '/content/drive/My Drive/char_sequences.txt'\n",
        "save_doc(sequences, out_filename)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Sequences: 1\n",
            "Total Sequences: 2\n",
            "Total Sequences: 3\n",
            "Total Sequences: 4\n",
            "Total Sequences: 5\n",
            "Total Sequences: 6\n",
            "Total Sequences: 7\n",
            "Total Sequences: 8\n",
            "Total Sequences: 9\n",
            "Total Sequences: 10\n",
            "Total Sequences: 11\n",
            "Total Sequences: 12\n",
            "Total Sequences: 13\n",
            "Total Sequences: 14\n",
            "Total Sequences: 15\n",
            "Total Sequences: 16\n",
            "Total Sequences: 17\n",
            "Total Sequences: 18\n",
            "Total Sequences: 19\n",
            "Total Sequences: 20\n",
            "Total Sequences: 21\n",
            "Total Sequences: 22\n",
            "Total Sequences: 23\n",
            "Total Sequences: 24\n",
            "Total Sequences: 25\n",
            "Total Sequences: 26\n",
            "Total Sequences: 27\n",
            "Total Sequences: 28\n",
            "Total Sequences: 29\n",
            "Total Sequences: 30\n",
            "Total Sequences: 31\n",
            "Total Sequences: 32\n",
            "Total Sequences: 33\n",
            "Total Sequences: 34\n",
            "Total Sequences: 35\n",
            "Total Sequences: 36\n",
            "Total Sequences: 37\n",
            "Total Sequences: 38\n",
            "Total Sequences: 39\n",
            "Total Sequences: 40\n",
            "Total Sequences: 41\n",
            "Total Sequences: 42\n",
            "Total Sequences: 43\n",
            "Total Sequences: 44\n",
            "Total Sequences: 45\n",
            "Total Sequences: 46\n",
            "Total Sequences: 47\n",
            "Total Sequences: 48\n",
            "Total Sequences: 49\n",
            "Total Sequences: 50\n",
            "Total Sequences: 51\n",
            "Total Sequences: 52\n",
            "Total Sequences: 53\n",
            "Total Sequences: 54\n",
            "Total Sequences: 55\n",
            "Total Sequences: 56\n",
            "Total Sequences: 57\n",
            "Total Sequences: 58\n",
            "Total Sequences: 59\n",
            "Total Sequences: 60\n",
            "Total Sequences: 61\n",
            "Total Sequences: 62\n",
            "Total Sequences: 63\n",
            "Total Sequences: 64\n",
            "Total Sequences: 65\n",
            "Total Sequences: 66\n",
            "Total Sequences: 67\n",
            "Total Sequences: 68\n",
            "Total Sequences: 69\n",
            "Total Sequences: 70\n",
            "Total Sequences: 71\n",
            "Total Sequences: 72\n",
            "Total Sequences: 73\n",
            "Total Sequences: 74\n",
            "Total Sequences: 75\n",
            "Total Sequences: 76\n",
            "Total Sequences: 77\n",
            "Total Sequences: 78\n",
            "Total Sequences: 79\n",
            "Total Sequences: 80\n",
            "Total Sequences: 81\n",
            "Total Sequences: 82\n",
            "Total Sequences: 83\n",
            "Total Sequences: 84\n",
            "Total Sequences: 85\n",
            "Total Sequences: 86\n",
            "Total Sequences: 87\n",
            "Total Sequences: 88\n",
            "Total Sequences: 89\n",
            "Total Sequences: 90\n",
            "Total Sequences: 91\n",
            "Total Sequences: 92\n",
            "Total Sequences: 93\n",
            "Total Sequences: 94\n",
            "Total Sequences: 95\n",
            "Total Sequences: 96\n",
            "Total Sequences: 97\n",
            "Total Sequences: 98\n",
            "Total Sequences: 99\n",
            "Total Sequences: 100\n",
            "Total Sequences: 101\n",
            "Total Sequences: 102\n",
            "Total Sequences: 103\n",
            "Total Sequences: 104\n",
            "Total Sequences: 105\n",
            "Total Sequences: 106\n",
            "Total Sequences: 107\n",
            "Total Sequences: 108\n",
            "Total Sequences: 109\n",
            "Total Sequences: 110\n",
            "Total Sequences: 111\n",
            "Total Sequences: 112\n",
            "Total Sequences: 113\n",
            "Total Sequences: 114\n",
            "Total Sequences: 115\n",
            "Total Sequences: 116\n",
            "Total Sequences: 117\n",
            "Total Sequences: 118\n",
            "Total Sequences: 119\n",
            "Total Sequences: 120\n",
            "Total Sequences: 121\n",
            "Total Sequences: 122\n",
            "Total Sequences: 123\n",
            "Total Sequences: 124\n",
            "Total Sequences: 125\n",
            "Total Sequences: 126\n",
            "Total Sequences: 127\n",
            "Total Sequences: 128\n",
            "Total Sequences: 129\n",
            "Total Sequences: 130\n",
            "Total Sequences: 131\n",
            "Total Sequences: 132\n",
            "Total Sequences: 133\n",
            "Total Sequences: 134\n",
            "Total Sequences: 135\n",
            "Total Sequences: 136\n",
            "Total Sequences: 137\n",
            "Total Sequences: 138\n",
            "Total Sequences: 139\n",
            "Total Sequences: 140\n",
            "Total Sequences: 141\n",
            "Total Sequences: 142\n",
            "Total Sequences: 143\n",
            "Total Sequences: 144\n",
            "Total Sequences: 145\n",
            "Total Sequences: 146\n",
            "Total Sequences: 147\n",
            "Total Sequences: 148\n",
            "Total Sequences: 149\n",
            "Total Sequences: 150\n",
            "Total Sequences: 151\n",
            "Total Sequences: 152\n",
            "Total Sequences: 153\n",
            "Total Sequences: 154\n",
            "Total Sequences: 155\n",
            "Total Sequences: 156\n",
            "Total Sequences: 157\n",
            "Total Sequences: 158\n",
            "Total Sequences: 159\n",
            "Total Sequences: 160\n",
            "Total Sequences: 161\n",
            "Total Sequences: 162\n",
            "Total Sequences: 163\n",
            "Total Sequences: 164\n",
            "Total Sequences: 165\n",
            "Total Sequences: 166\n",
            "Total Sequences: 167\n",
            "Total Sequences: 168\n",
            "Total Sequences: 169\n",
            "Total Sequences: 170\n",
            "Total Sequences: 171\n",
            "Total Sequences: 172\n",
            "Total Sequences: 173\n",
            "Total Sequences: 174\n",
            "Total Sequences: 175\n",
            "Total Sequences: 176\n",
            "Total Sequences: 177\n",
            "Total Sequences: 178\n",
            "Total Sequences: 179\n",
            "Total Sequences: 180\n",
            "Total Sequences: 181\n",
            "Total Sequences: 182\n",
            "Total Sequences: 183\n",
            "Total Sequences: 184\n",
            "Total Sequences: 185\n",
            "Total Sequences: 186\n",
            "Total Sequences: 187\n",
            "Total Sequences: 188\n",
            "Total Sequences: 189\n",
            "Total Sequences: 190\n",
            "Total Sequences: 191\n",
            "Total Sequences: 192\n",
            "Total Sequences: 193\n",
            "Total Sequences: 194\n",
            "Total Sequences: 195\n",
            "Total Sequences: 196\n",
            "Total Sequences: 197\n",
            "Total Sequences: 198\n",
            "Total Sequences: 199\n",
            "Total Sequences: 200\n",
            "Total Sequences: 201\n",
            "Total Sequences: 202\n",
            "Total Sequences: 203\n",
            "Total Sequences: 204\n",
            "Total Sequences: 205\n",
            "Total Sequences: 206\n",
            "Total Sequences: 207\n",
            "Total Sequences: 208\n",
            "Total Sequences: 209\n",
            "Total Sequences: 210\n",
            "Total Sequences: 211\n",
            "Total Sequences: 212\n",
            "Total Sequences: 213\n",
            "Total Sequences: 214\n",
            "Total Sequences: 215\n",
            "Total Sequences: 216\n",
            "Total Sequences: 217\n",
            "Total Sequences: 218\n",
            "Total Sequences: 219\n",
            "Total Sequences: 220\n",
            "Total Sequences: 221\n",
            "Total Sequences: 222\n",
            "Total Sequences: 223\n",
            "Total Sequences: 224\n",
            "Total Sequences: 225\n",
            "Total Sequences: 226\n",
            "Total Sequences: 227\n",
            "Total Sequences: 228\n",
            "Total Sequences: 229\n",
            "Total Sequences: 230\n",
            "Total Sequences: 231\n",
            "Total Sequences: 232\n",
            "Total Sequences: 233\n",
            "Total Sequences: 234\n",
            "Total Sequences: 235\n",
            "Total Sequences: 236\n",
            "Total Sequences: 237\n",
            "Total Sequences: 238\n",
            "Total Sequences: 239\n",
            "Total Sequences: 240\n",
            "Total Sequences: 241\n",
            "Total Sequences: 242\n",
            "Total Sequences: 243\n",
            "Total Sequences: 244\n",
            "Total Sequences: 245\n",
            "Total Sequences: 246\n",
            "Total Sequences: 247\n",
            "Total Sequences: 248\n",
            "Total Sequences: 249\n",
            "Total Sequences: 250\n",
            "Total Sequences: 251\n",
            "Total Sequences: 252\n",
            "Total Sequences: 253\n",
            "Total Sequences: 254\n",
            "Total Sequences: 255\n",
            "Total Sequences: 256\n",
            "Total Sequences: 257\n",
            "Total Sequences: 258\n",
            "Total Sequences: 259\n",
            "Total Sequences: 260\n",
            "Total Sequences: 261\n",
            "Total Sequences: 262\n",
            "Total Sequences: 263\n",
            "Total Sequences: 264\n",
            "Total Sequences: 265\n",
            "Total Sequences: 266\n",
            "Total Sequences: 267\n",
            "Total Sequences: 268\n",
            "Total Sequences: 269\n",
            "Total Sequences: 270\n",
            "Total Sequences: 271\n",
            "Total Sequences: 272\n",
            "Total Sequences: 273\n",
            "Total Sequences: 274\n",
            "Total Sequences: 275\n",
            "Total Sequences: 276\n",
            "Total Sequences: 277\n",
            "Total Sequences: 278\n",
            "Total Sequences: 279\n",
            "Total Sequences: 280\n",
            "Total Sequences: 281\n",
            "Total Sequences: 282\n",
            "Total Sequences: 283\n",
            "Total Sequences: 284\n",
            "Total Sequences: 285\n",
            "Total Sequences: 286\n",
            "Total Sequences: 287\n",
            "Total Sequences: 288\n",
            "Total Sequences: 289\n",
            "Total Sequences: 290\n",
            "Total Sequences: 291\n",
            "Total Sequences: 292\n",
            "Total Sequences: 293\n",
            "Total Sequences: 294\n",
            "Total Sequences: 295\n",
            "Total Sequences: 296\n",
            "Total Sequences: 297\n",
            "Total Sequences: 298\n",
            "Total Sequences: 299\n",
            "Total Sequences: 300\n",
            "Total Sequences: 301\n",
            "Total Sequences: 302\n",
            "Total Sequences: 303\n",
            "Total Sequences: 304\n",
            "Total Sequences: 305\n",
            "Total Sequences: 306\n",
            "Total Sequences: 307\n",
            "Total Sequences: 308\n",
            "Total Sequences: 309\n",
            "Total Sequences: 310\n",
            "Total Sequences: 311\n",
            "Total Sequences: 312\n",
            "Total Sequences: 313\n",
            "Total Sequences: 314\n",
            "Total Sequences: 315\n",
            "Total Sequences: 316\n",
            "Total Sequences: 317\n",
            "Total Sequences: 318\n",
            "Total Sequences: 319\n",
            "Total Sequences: 320\n",
            "Total Sequences: 321\n",
            "Total Sequences: 322\n",
            "Total Sequences: 323\n",
            "Total Sequences: 324\n",
            "Total Sequences: 325\n",
            "Total Sequences: 326\n",
            "Total Sequences: 327\n",
            "Total Sequences: 328\n",
            "Total Sequences: 329\n",
            "Total Sequences: 330\n",
            "Total Sequences: 331\n",
            "Total Sequences: 332\n",
            "Total Sequences: 333\n",
            "Total Sequences: 334\n",
            "Total Sequences: 335\n",
            "Total Sequences: 336\n",
            "Total Sequences: 337\n",
            "Total Sequences: 338\n",
            "Total Sequences: 339\n",
            "Total Sequences: 340\n",
            "Total Sequences: 341\n",
            "Total Sequences: 342\n",
            "Total Sequences: 343\n",
            "Total Sequences: 344\n",
            "Total Sequences: 345\n",
            "Total Sequences: 346\n",
            "Total Sequences: 347\n",
            "Total Sequences: 348\n",
            "Total Sequences: 349\n",
            "Total Sequences: 350\n",
            "Total Sequences: 351\n",
            "Total Sequences: 352\n",
            "Total Sequences: 353\n",
            "Total Sequences: 354\n",
            "Total Sequences: 355\n",
            "Total Sequences: 356\n",
            "Total Sequences: 357\n",
            "Total Sequences: 358\n",
            "Total Sequences: 359\n",
            "Total Sequences: 360\n",
            "Total Sequences: 361\n",
            "Total Sequences: 362\n",
            "Total Sequences: 363\n",
            "Total Sequences: 364\n",
            "Total Sequences: 365\n",
            "Total Sequences: 366\n",
            "Total Sequences: 367\n",
            "Total Sequences: 368\n",
            "Total Sequences: 369\n",
            "Total Sequences: 370\n",
            "Total Sequences: 371\n",
            "Total Sequences: 372\n",
            "Total Sequences: 373\n",
            "Total Sequences: 374\n",
            "Total Sequences: 375\n",
            "Total Sequences: 376\n",
            "Total Sequences: 377\n",
            "Total Sequences: 378\n",
            "Total Sequences: 379\n",
            "Total Sequences: 380\n",
            "Total Sequences: 381\n",
            "Total Sequences: 382\n",
            "Total Sequences: 383\n",
            "Total Sequences: 384\n",
            "Total Sequences: 385\n",
            "Total Sequences: 386\n",
            "Total Sequences: 387\n",
            "Total Sequences: 388\n",
            "Total Sequences: 389\n",
            "Total Sequences: 390\n",
            "Total Sequences: 391\n",
            "Total Sequences: 392\n",
            "Total Sequences: 393\n",
            "Total Sequences: 394\n",
            "Total Sequences: 395\n",
            "Total Sequences: 396\n",
            "Total Sequences: 397\n",
            "Total Sequences: 398\n",
            "Total Sequences: 399\n",
            "Total Sequences: 400\n",
            "Total Sequences: 401\n",
            "Total Sequences: 402\n",
            "Total Sequences: 403\n",
            "Total Sequences: 404\n",
            "Total Sequences: 405\n",
            "Total Sequences: 406\n",
            "Total Sequences: 407\n",
            "Total Sequences: 408\n",
            "Total Sequences: 409\n",
            "Total Sequences: 410\n",
            "Total Sequences: 411\n",
            "Total Sequences: 412\n",
            "Total Sequences: 413\n",
            "Total Sequences: 414\n",
            "Total Sequences: 415\n",
            "Total Sequences: 416\n",
            "Total Sequences: 417\n",
            "Total Sequences: 418\n",
            "Total Sequences: 419\n",
            "Total Sequences: 420\n",
            "Total Sequences: 421\n",
            "Total Sequences: 422\n",
            "Total Sequences: 423\n",
            "Total Sequences: 424\n",
            "Total Sequences: 425\n",
            "Total Sequences: 426\n",
            "Total Sequences: 427\n",
            "Total Sequences: 428\n",
            "Total Sequences: 429\n",
            "Total Sequences: 430\n",
            "Total Sequences: 431\n",
            "Total Sequences: 432\n",
            "Total Sequences: 433\n",
            "Total Sequences: 434\n",
            "Total Sequences: 435\n",
            "Total Sequences: 436\n",
            "Total Sequences: 437\n",
            "Total Sequences: 438\n",
            "Total Sequences: 439\n",
            "Total Sequences: 440\n",
            "Total Sequences: 441\n",
            "Total Sequences: 442\n",
            "Total Sequences: 443\n",
            "Total Sequences: 444\n",
            "Total Sequences: 445\n",
            "Total Sequences: 446\n",
            "Total Sequences: 447\n",
            "Total Sequences: 448\n",
            "Total Sequences: 449\n",
            "Total Sequences: 450\n",
            "Total Sequences: 451\n",
            "Total Sequences: 452\n",
            "Total Sequences: 453\n",
            "Total Sequences: 454\n",
            "Total Sequences: 455\n",
            "Total Sequences: 456\n",
            "Total Sequences: 457\n",
            "Total Sequences: 458\n",
            "Total Sequences: 459\n",
            "Total Sequences: 460\n",
            "Total Sequences: 461\n",
            "Total Sequences: 462\n",
            "Total Sequences: 463\n",
            "Total Sequences: 464\n",
            "Total Sequences: 465\n",
            "Total Sequences: 466\n",
            "Total Sequences: 467\n",
            "Total Sequences: 468\n",
            "Total Sequences: 469\n",
            "Total Sequences: 470\n",
            "Total Sequences: 471\n",
            "Total Sequences: 472\n",
            "Total Sequences: 473\n",
            "Total Sequences: 474\n",
            "Total Sequences: 475\n",
            "Total Sequences: 476\n",
            "Total Sequences: 477\n",
            "Total Sequences: 478\n",
            "Total Sequences: 479\n",
            "Total Sequences: 480\n",
            "Total Sequences: 481\n",
            "Total Sequences: 482\n",
            "Total Sequences: 483\n",
            "Total Sequences: 484\n",
            "Total Sequences: 485\n",
            "Total Sequences: 486\n",
            "Total Sequences: 487\n",
            "Total Sequences: 488\n",
            "Total Sequences: 489\n",
            "Total Sequences: 490\n",
            "Total Sequences: 491\n",
            "Total Sequences: 492\n",
            "Total Sequences: 493\n",
            "Total Sequences: 494\n",
            "Total Sequences: 495\n",
            "Total Sequences: 496\n",
            "Total Sequences: 497\n",
            "Total Sequences: 498\n",
            "Total Sequences: 499\n",
            "Total Sequences: 500\n",
            "Total Sequences: 501\n",
            "Total Sequences: 502\n",
            "Total Sequences: 503\n",
            "Total Sequences: 504\n",
            "Total Sequences: 505\n",
            "Total Sequences: 506\n",
            "Total Sequences: 507\n",
            "Total Sequences: 508\n",
            "Total Sequences: 509\n",
            "Total Sequences: 510\n",
            "Total Sequences: 511\n",
            "Total Sequences: 512\n",
            "Total Sequences: 513\n",
            "Total Sequences: 514\n",
            "Total Sequences: 515\n",
            "Total Sequences: 516\n",
            "Total Sequences: 517\n",
            "Total Sequences: 518\n",
            "Total Sequences: 519\n",
            "Total Sequences: 520\n",
            "Total Sequences: 521\n",
            "Total Sequences: 522\n",
            "Total Sequences: 523\n",
            "Total Sequences: 524\n",
            "Total Sequences: 525\n",
            "Total Sequences: 526\n",
            "Total Sequences: 527\n",
            "Total Sequences: 528\n",
            "Total Sequences: 529\n",
            "Total Sequences: 530\n",
            "Total Sequences: 531\n",
            "Total Sequences: 532\n",
            "Total Sequences: 533\n",
            "Total Sequences: 534\n",
            "Total Sequences: 535\n",
            "Total Sequences: 536\n",
            "Total Sequences: 537\n",
            "Total Sequences: 538\n",
            "Total Sequences: 539\n",
            "Total Sequences: 540\n",
            "Total Sequences: 541\n",
            "Total Sequences: 542\n",
            "Total Sequences: 543\n",
            "Total Sequences: 544\n",
            "Total Sequences: 545\n",
            "Total Sequences: 546\n",
            "Total Sequences: 547\n",
            "Total Sequences: 548\n",
            "Total Sequences: 549\n",
            "Total Sequences: 550\n",
            "Total Sequences: 551\n",
            "Total Sequences: 552\n",
            "Total Sequences: 553\n",
            "Total Sequences: 554\n",
            "Total Sequences: 555\n",
            "Total Sequences: 556\n",
            "Total Sequences: 557\n",
            "Total Sequences: 558\n",
            "Total Sequences: 559\n",
            "Total Sequences: 560\n",
            "Total Sequences: 561\n",
            "Total Sequences: 562\n",
            "Total Sequences: 563\n",
            "Total Sequences: 564\n",
            "Total Sequences: 565\n",
            "Total Sequences: 566\n",
            "Total Sequences: 567\n",
            "Total Sequences: 568\n",
            "Total Sequences: 569\n",
            "Total Sequences: 570\n",
            "Total Sequences: 571\n",
            "Total Sequences: 572\n",
            "Total Sequences: 573\n",
            "Total Sequences: 574\n",
            "Total Sequences: 575\n",
            "Total Sequences: 576\n",
            "Total Sequences: 577\n",
            "Total Sequences: 578\n",
            "Total Sequences: 579\n",
            "Total Sequences: 580\n",
            "Total Sequences: 581\n",
            "Total Sequences: 582\n",
            "Total Sequences: 583\n",
            "Total Sequences: 584\n",
            "Total Sequences: 585\n",
            "Total Sequences: 586\n",
            "Total Sequences: 587\n",
            "Total Sequences: 588\n",
            "Total Sequences: 589\n",
            "Total Sequences: 590\n",
            "Total Sequences: 591\n",
            "Total Sequences: 592\n",
            "Total Sequences: 593\n",
            "Total Sequences: 594\n",
            "Total Sequences: 595\n",
            "Total Sequences: 596\n",
            "Total Sequences: 597\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8uAuvqwF-_-M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#load\n",
        "in_filename = '/content/drive/My Drive/char_sequences.txt'\n",
        "raw_text = load_doc(in_filename)\n",
        "lines = raw_text.split('\\n')\n",
        "\n",
        "chars = sorted(list(set(raw_text)))\n",
        "mapping = dict((c, i) for i, c in enumerate(chars))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZx8wPbx-_-M",
        "colab_type": "code",
        "outputId": "ef0baa61-1e8b-4c7f-ae45-72b20b251aae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "sequences = list()\n",
        "for line in lines:\n",
        "    #integer encode line\n",
        "    encoded_seq = [mapping[char] for char in line]\n",
        "    #store\n",
        "    sequences.append(encoded_seq)\n",
        "\n",
        "#vocabulary size\n",
        "vocab_size = len(mapping)\n",
        "print('Vocabulary Size: %d' % vocab_size)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary Size: 37\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mgyEIrZe-_-M",
        "colab_type": "code",
        "outputId": "ac9decf9-91ef-4645-a368-028bd0e1e0c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "sequences = array(sequences)\n",
        "X, y = sequences[:,:-1], sequences[:,-1]\n",
        "\n",
        "sequences = [to_categorical(x, num_classes=vocab_size) for x in X]\n",
        "X = array(sequences)\n",
        "y = to_categorical(y, num_classes=vocab_size)\n",
        "X.shape"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(597, 12, 37)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IkHCwDQY-_-M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define the model\n",
        "def define_model(X):\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(75, input_shape=(X.shape[1], X.shape[2])))\n",
        "    model.add(Dense(vocab_size, activation='softmax'))\n",
        "    # compile model\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    # summarize defined model\n",
        "    model.summary()\n",
        "   # plot_model(model, to_file='model.png', show_shapes=True)\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6pDr42Id-_-M",
        "colab_type": "code",
        "outputId": "e44a7ee0-73fe-4484-fb77-a0ea3481e0d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4041
        }
      },
      "source": [
        "# define model\n",
        "model = define_model(X)\n",
        "# fit model\n",
        "model.fit(X, y, epochs=100, verbose=2)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_1 (LSTM)                (None, 75)                33900     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 37)                2812      \n",
            "=================================================================\n",
            "Total params: 36,712\n",
            "Trainable params: 36,712\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Epoch 1/100\n",
            " - 1s - loss: 3.5734 - acc: 0.1055\n",
            "Epoch 2/100\n",
            " - 0s - loss: 3.2540 - acc: 0.1759\n",
            "Epoch 3/100\n",
            " - 0s - loss: 3.0296 - acc: 0.1759\n",
            "Epoch 4/100\n",
            " - 0s - loss: 2.9729 - acc: 0.1742\n",
            "Epoch 5/100\n",
            " - 0s - loss: 2.9498 - acc: 0.1742\n",
            "Epoch 6/100\n",
            " - 0s - loss: 2.9371 - acc: 0.1742\n",
            "Epoch 7/100\n",
            " - 0s - loss: 2.9235 - acc: 0.1742\n",
            "Epoch 8/100\n",
            " - 0s - loss: 2.9098 - acc: 0.1742\n",
            "Epoch 9/100\n",
            " - 0s - loss: 2.8945 - acc: 0.1826\n",
            "Epoch 10/100\n",
            " - 0s - loss: 2.8710 - acc: 0.1792\n",
            "Epoch 11/100\n",
            " - 0s - loss: 2.8485 - acc: 0.1943\n",
            "Epoch 12/100\n",
            " - 0s - loss: 2.8179 - acc: 0.1943\n",
            "Epoch 13/100\n",
            " - 0s - loss: 2.7963 - acc: 0.2094\n",
            "Epoch 14/100\n",
            " - 0s - loss: 2.7779 - acc: 0.2144\n",
            "Epoch 15/100\n",
            " - 0s - loss: 2.7274 - acc: 0.2295\n",
            "Epoch 16/100\n",
            " - 0s - loss: 2.6967 - acc: 0.2596\n",
            "Epoch 17/100\n",
            " - 0s - loss: 2.6599 - acc: 0.2379\n",
            "Epoch 18/100\n",
            " - 0s - loss: 2.6369 - acc: 0.2613\n",
            "Epoch 19/100\n",
            " - 0s - loss: 2.6005 - acc: 0.3065\n",
            "Epoch 20/100\n",
            " - 0s - loss: 2.5625 - acc: 0.2898\n",
            "Epoch 21/100\n",
            " - 0s - loss: 2.5449 - acc: 0.2931\n",
            "Epoch 22/100\n",
            " - 0s - loss: 2.5089 - acc: 0.2848\n",
            "Epoch 23/100\n",
            " - 0s - loss: 2.4527 - acc: 0.3250\n",
            "Epoch 24/100\n",
            " - 0s - loss: 2.4300 - acc: 0.3451\n",
            "Epoch 25/100\n",
            " - 0s - loss: 2.4029 - acc: 0.3216\n",
            "Epoch 26/100\n",
            " - 0s - loss: 2.3523 - acc: 0.3417\n",
            "Epoch 27/100\n",
            " - 0s - loss: 2.3144 - acc: 0.3501\n",
            "Epoch 28/100\n",
            " - 0s - loss: 2.2754 - acc: 0.3501\n",
            "Epoch 29/100\n",
            " - 0s - loss: 2.2438 - acc: 0.3819\n",
            "Epoch 30/100\n",
            " - 0s - loss: 2.1986 - acc: 0.3886\n",
            "Epoch 31/100\n",
            " - 0s - loss: 2.1918 - acc: 0.3685\n",
            "Epoch 32/100\n",
            " - 0s - loss: 2.1498 - acc: 0.3936\n",
            "Epoch 33/100\n",
            " - 0s - loss: 2.0837 - acc: 0.4238\n",
            "Epoch 34/100\n",
            " - 0s - loss: 2.0644 - acc: 0.4154\n",
            "Epoch 35/100\n",
            " - 0s - loss: 2.0284 - acc: 0.4422\n",
            "Epoch 36/100\n",
            " - 0s - loss: 1.9883 - acc: 0.4355\n",
            "Epoch 37/100\n",
            " - 0s - loss: 1.9380 - acc: 0.4640\n",
            "Epoch 38/100\n",
            " - 0s - loss: 1.9139 - acc: 0.4456\n",
            "Epoch 39/100\n",
            " - 0s - loss: 1.8712 - acc: 0.4807\n",
            "Epoch 40/100\n",
            " - 0s - loss: 1.8605 - acc: 0.4891\n",
            "Epoch 41/100\n",
            " - 0s - loss: 1.8201 - acc: 0.4841\n",
            "Epoch 42/100\n",
            " - 0s - loss: 1.7541 - acc: 0.5092\n",
            "Epoch 43/100\n",
            " - 0s - loss: 1.7128 - acc: 0.5327\n",
            "Epoch 44/100\n",
            " - 0s - loss: 1.6711 - acc: 0.5193\n",
            "Epoch 45/100\n",
            " - 0s - loss: 1.6433 - acc: 0.5360\n",
            "Epoch 46/100\n",
            " - 0s - loss: 1.6170 - acc: 0.5410\n",
            "Epoch 47/100\n",
            " - 0s - loss: 1.5820 - acc: 0.5712\n",
            "Epoch 48/100\n",
            " - 0s - loss: 1.5296 - acc: 0.5796\n",
            "Epoch 49/100\n",
            " - 0s - loss: 1.5093 - acc: 0.5913\n",
            "Epoch 50/100\n",
            " - 0s - loss: 1.4374 - acc: 0.6064\n",
            "Epoch 51/100\n",
            " - 0s - loss: 1.4185 - acc: 0.6265\n",
            "Epoch 52/100\n",
            " - 0s - loss: 1.3896 - acc: 0.6231\n",
            "Epoch 53/100\n",
            " - 0s - loss: 1.3342 - acc: 0.6415\n",
            "Epoch 54/100\n",
            " - 0s - loss: 1.2997 - acc: 0.6583\n",
            "Epoch 55/100\n",
            " - 0s - loss: 1.2639 - acc: 0.6834\n",
            "Epoch 56/100\n",
            " - 0s - loss: 1.2304 - acc: 0.7052\n",
            "Epoch 57/100\n",
            " - 0s - loss: 1.1851 - acc: 0.7203\n",
            "Epoch 58/100\n",
            " - 0s - loss: 1.1539 - acc: 0.7052\n",
            "Epoch 59/100\n",
            " - 0s - loss: 1.1392 - acc: 0.7236\n",
            "Epoch 60/100\n",
            " - 0s - loss: 1.1015 - acc: 0.7353\n",
            "Epoch 61/100\n",
            " - 0s - loss: 1.0321 - acc: 0.7605\n",
            "Epoch 62/100\n",
            " - 0s - loss: 0.9957 - acc: 0.7755\n",
            "Epoch 63/100\n",
            " - 0s - loss: 0.9972 - acc: 0.7822\n",
            "Epoch 64/100\n",
            " - 0s - loss: 0.9408 - acc: 0.7806\n",
            "Epoch 65/100\n",
            " - 0s - loss: 0.9126 - acc: 0.8074\n",
            "Epoch 66/100\n",
            " - 0s - loss: 0.8701 - acc: 0.8191\n",
            "Epoch 67/100\n",
            " - 0s - loss: 0.8410 - acc: 0.8275\n",
            "Epoch 68/100\n",
            " - 0s - loss: 0.8078 - acc: 0.8409\n",
            "Epoch 69/100\n",
            " - 0s - loss: 0.7619 - acc: 0.8660\n",
            "Epoch 70/100\n",
            " - 0s - loss: 0.7526 - acc: 0.8610\n",
            "Epoch 71/100\n",
            " - 0s - loss: 0.7158 - acc: 0.8827\n",
            "Epoch 72/100\n",
            " - 0s - loss: 0.6923 - acc: 0.8710\n",
            "Epoch 73/100\n",
            " - 0s - loss: 0.6553 - acc: 0.8945\n",
            "Epoch 74/100\n",
            " - 0s - loss: 0.6541 - acc: 0.8928\n",
            "Epoch 75/100\n",
            " - 0s - loss: 0.6002 - acc: 0.9280\n",
            "Epoch 76/100\n",
            " - 0s - loss: 0.5604 - acc: 0.9246\n",
            "Epoch 77/100\n",
            " - 0s - loss: 0.5325 - acc: 0.9263\n",
            "Epoch 78/100\n",
            " - 0s - loss: 0.5032 - acc: 0.9397\n",
            "Epoch 79/100\n",
            " - 0s - loss: 0.4924 - acc: 0.9464\n",
            "Epoch 80/100\n",
            " - 0s - loss: 0.4666 - acc: 0.9514\n",
            "Epoch 81/100\n",
            " - 0s - loss: 0.4494 - acc: 0.9497\n",
            "Epoch 82/100\n",
            " - 0s - loss: 0.4291 - acc: 0.9548\n",
            "Epoch 83/100\n",
            " - 0s - loss: 0.4202 - acc: 0.9648\n",
            "Epoch 84/100\n",
            " - 0s - loss: 0.3951 - acc: 0.9648\n",
            "Epoch 85/100\n",
            " - 0s - loss: 0.3833 - acc: 0.9715\n",
            "Epoch 86/100\n",
            " - 0s - loss: 0.3671 - acc: 0.9715\n",
            "Epoch 87/100\n",
            " - 0s - loss: 0.3559 - acc: 0.9749\n",
            "Epoch 88/100\n",
            " - 0s - loss: 0.3201 - acc: 0.9849\n",
            "Epoch 89/100\n",
            " - 0s - loss: 0.3001 - acc: 0.9816\n",
            "Epoch 90/100\n",
            " - 0s - loss: 0.2863 - acc: 0.9899\n",
            "Epoch 91/100\n",
            " - 0s - loss: 0.2767 - acc: 0.9866\n",
            "Epoch 92/100\n",
            " - 0s - loss: 0.2644 - acc: 0.9883\n",
            "Epoch 93/100\n",
            " - 0s - loss: 0.2455 - acc: 0.9916\n",
            "Epoch 94/100\n",
            " - 0s - loss: 0.2321 - acc: 0.9950\n",
            "Epoch 95/100\n",
            " - 0s - loss: 0.2238 - acc: 0.9916\n",
            "Epoch 96/100\n",
            " - 0s - loss: 0.2113 - acc: 0.9916\n",
            "Epoch 97/100\n",
            " - 0s - loss: 0.2054 - acc: 0.9933\n",
            "Epoch 98/100\n",
            " - 0s - loss: 0.1980 - acc: 0.9933\n",
            "Epoch 99/100\n",
            " - 0s - loss: 0.1874 - acc: 0.9950\n",
            "Epoch 100/100\n",
            " - 0s - loss: 0.1794 - acc: 0.9950\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fd25815a6d8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7X9k2Cl-_-M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save the model to file\n",
        "model.save('model.h5')\n",
        "# save the mapping\n",
        "dump(mapping, open('mapping.pkl', 'wb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_FkgurRS-_-M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pickle import load\n",
        "from keras.models import load_model\n",
        "from keras.utils import to_categorical\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# load the model\n",
        "model = load_model('model.h5')\n",
        "# load the mapping\n",
        "mapping = load(open('mapping.pkl', 'rb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIVeKHy9-_-M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# generate a sequence of characters with a language model\n",
        "def generate_seq(model, mapping, seq_length, seed_text, n_chars):\n",
        "    in_text = seed_text\n",
        "    len(in_text)\n",
        "    # generate a fixed number of characters\n",
        "    for _ in range(n_chars):\n",
        "        # encode the characters as integers\n",
        "        encoded = [mapping[char] for char in in_text]\n",
        "        # truncate sequences to a fixed length\n",
        "        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
        "        # one hot encode\n",
        "        encoded = to_categorical(encoded, num_classes=len(mapping))\n",
        "        encoded.shape\n",
        "        #encoded = encoded.reshape( 1,encoded.shape[0], encoded.shape[1])\n",
        "        # predict character\n",
        "        yhat = model.predict_classes(encoded, verbose=0)\n",
        "        # reverse map integer to character\n",
        "        out_char = ''\n",
        "        for char, index in mapping.items():\n",
        "            if index == yhat:\n",
        "                out_char = char\n",
        "                break\n",
        "            # append to input\n",
        "        in_text += out_char\n",
        "    return in_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bs5ZqmYn-_-M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "cef3c9be-aede-4e9c-824b-2e58298d582b"
      },
      "source": [
        "# test start of rhyme\n",
        "print(generate_seq(model, mapping, 12, 'From fairest', 100))\n",
        "# test mid-line\n",
        "print(generate_seq(model, mapping, 12, 'Making a famine', 70))\n",
        "# test not in original\n",
        "print(generate_seq(model, mapping, 12, 'style', 70))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "From fairest creatures we desire increase, That thereby beauty's rose might never die, But as the riper should b\n",
            "Making a famine where abundance lies, Thy self thy foe, to thy sweet self too cruel: \n",
            "style old sefe shisrermgtutmeve ndd --l-u,iP this wdoerocrise thoy corut, t\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qGbNBL64-_-M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}