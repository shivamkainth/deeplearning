{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Day 3 CNN Classification .ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shivamkainth/deeplearning/blob/master/Day_3_CNN_Classification_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LoP_IgEfW5j4",
        "colab_type": "text"
      },
      "source": [
        "# Deep Learning and Applications : Joint Faculty Development Programme\n",
        "# May 27 -31, 2019 \n",
        "\n",
        "**Participating Academies : IIT Roorkee ,  MNIT Jaipur, NIT Patna, PDPM IIITDM Jabalpur  **\n",
        "\n",
        "**Prinicipal Coordinating Academies : IIT Roorkee and PDPM IIITDM Jabalpur**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "haRAoQFmW5j4",
        "colab_type": "text"
      },
      "source": [
        "# Tutorial 3: Introduction to Convolution Neural Networks\n",
        "** In this tutorial, we provide you a brief introduction to Convolution Neural Networks(CNNs). You will be able to build a CNN model that can be used for classification.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zd-rTU7zW5j4",
        "colab_type": "text"
      },
      "source": [
        "Convolution Neural Networks are mainly use for large size input data such as Image data. \n",
        "* Convolution Neural Networks (CNNs) use parameter sharing.\n",
        "* Small pattern detectors called filters are used to convolve over the entire image.\n",
        "* These filters are learned through NN training in the same way as in fully connected networks.\n",
        "* Just like a hidden layer in a fully connected layer, convolution layers are used in CNNs.\n",
        "* To handle large size of image data, pooling layers are introduced.\n",
        "* Normalization layers were used in early CNN architectures, but due to their minimal impact, they are not much used in the present CNNs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFfIPFgVW5j4",
        "colab_type": "text"
      },
      "source": [
        "** Today we are going to see a simple 3 convolution layers convolution neural network. We are going to use PlantVillage leaf \n",
        "dataset. It contains 38 classes dataset. From which we are just going to use 4 classes of Apple. The objective of the work is to Diseases classification in Apple leaves.**\n",
        "\n",
        "Dataset Link : :https://github.com/spMohanty/PlantVillage-Dataset/tree/master/raw/color\n",
        "We split the dataset into training validation and testing sample"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHE4fhkccHCE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "40bcb968-f7c3-45e6-c50d-da559dc69d90"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqXz6wGDW5j4",
        "colab_type": "text"
      },
      "source": [
        "### A. Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sieSFRN-W5j4",
        "colab_type": "text"
      },
      "source": [
        "** A1. Import required Libraries **\n",
        "* From keras library we are going to use image preprocessing task, to normalize the image pixel values in between 0 to 1.\n",
        "* Model is imported to load variuos Neural NEtwrok models such as Sequential.\n",
        "* We are going to use Stochastic Gradient Descent(SGD) as a optimizer \n",
        "* Keras layers such as Dense, Flatten, Conv2D and MaxPooling is used to implement the CNN model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-rwHySzsW5j4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import keras\n",
        "from keras import models\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.preprocessing import image\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Model\n",
        "from keras.optimizers import SGD\n",
        "from keras import layers\n",
        "from keras.layers import  Dense, Flatten, Conv2D, MaxPooling2D\n",
        "from keras import Input\n",
        "                                                                                                            \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MrjDj6VW5j4",
        "colab_type": "text"
      },
      "source": [
        "** A2. Loading the training and testing data and defining the basic parameters **\n",
        "* We are resizing the input image to 64 * 64\n",
        "\n",
        "* In the dataset :\n",
        "    Training Set : 70% \n",
        "    Validation Set : 20%\n",
        "    Test Set : 10%"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Mf3FKjFW5lc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Normalize training and validation data in the range of 0 to 1\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Read the training sample and set the batch size \n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        '/content/gdrive/My Drive/Colab Notebooks/plant_village/train/',\n",
        "        target_size=(64, 64),\n",
        "        batch_size=16,\n",
        "        class_mode='categorical')\n",
        "\n",
        "# Read Validation data from directory and define target size with batch size\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "        '/content/gdrive/My Drive/Colab Notebooks/plant_village/val/',\n",
        "        target_size=(64, 64),\n",
        "        batch_size=16,\n",
        "        class_mode='categorical',\n",
        "        shuffle=False)\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "        '/content/gdrive/My Drive/Colab Notebooks/plant_village/test/',\n",
        "        target_size=(64, 64),\n",
        "        batch_size=1,\n",
        "        class_mode='categorical',\n",
        "        shuffle=False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FMfDAl8PW5lc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from PIL import Image\n",
        "import os, sys\n",
        "\n",
        "\n",
        "dirs = os.listdir(\"/content/gdrive/My Drive/Colab Notebooks/plant_village/val/Apple_Frogeye_Spot/\")\n",
        "def resize():\n",
        "    for item in dirs: \n",
        "        if os.path.isfile(\"/content/gdrive/My Drive/Colab Notebooks/plant_village/val/Apple_Frogeye_Spot/\"+item):\n",
        "            im = Image.open(\"/content/gdrive/My Drive/Colab Notebooks/plant_village/val/Apple_Frogeye_Spot/\"+item)\n",
        "            f, e = os.path.splitext(\"/content/gdrive/My Drive/Colab Notebooks/plant_village/val/Apple_Frogeye_Spot/\"+item)\n",
        "            imResize = im.resize((64, 64), Image.ANTIALIAS)\n",
        "            imResize.save(f + '.jpg', 'JPEG', quality=90)\n",
        "\n",
        "resize()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "miHO7npOW5lc",
        "colab_type": "text"
      },
      "source": [
        "### B. Model Building\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iyxRyKEWW5lc",
        "colab_type": "text"
      },
      "source": [
        "* We are going to use 2 convolution layers with 3*3 filer and relu as an activation function\n",
        "* Then max pooling layer with 2*2 filter is used\n",
        "* After that we are going to use Flatten layer\n",
        "* Then Dense layer is used with relu function\n",
        "* In the output layer softmax function is used with 4 neurons as we have four class dataset.\n",
        "* model.summary() is used to check the overall architecture of the model with number of learnable parameters in each \n",
        "\n",
        "#### B1. Model Definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTV4s3VCW5lc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create the model\n",
        "model = models.Sequential()\n",
        "# Add new layers\n",
        "model.add(Conv2D(128, kernel_size=(3,3), activation = 'relu', input_shape=(64,64,3)))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Conv2D(64, kernel_size=(3,3), activation = 'relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Conv2D(64, kernel_size=(3,3), activation = 'relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(32, activation='relu'))\n",
        "model.add(layers.Dense(4, activation='softmax'))\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dp8fcKXzW5lc",
        "colab_type": "text"
      },
      "source": [
        "### B2. Compile the model with SGD(Stochastic Gradient Descent) and train it with 10 epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "S-Yw2YVFW5lc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sgd = SGD(lr=0.001,decay=1e-6, momentum=0.9, nesterov=True)\n",
        "# We are going to use accuracy metrics and cross entropy loss as performance parameters\n",
        "model.compile(sgd, loss='categorical_crossentropy', metrics=['acc'])\n",
        "# Train the model \n",
        "history = model.fit_generator(train_generator, \n",
        "      steps_per_epoch=train_generator.samples/train_generator.batch_size,\n",
        "      epochs=10,\n",
        "      validation_data=validation_generator,\n",
        "      validation_steps=validation_generator.samples/validation_generator.batch_size,\n",
        "      verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bnLh4RoW5lc",
        "colab_type": "text"
      },
      "source": [
        "### B3. Saving the model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ya8mWE2zW5lc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save('cnn_classification.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6C7TDR-8W5lc",
        "colab_type": "text"
      },
      "source": [
        "### B4. Loading the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVf9LNmAW5lc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = models.load_model('cnn_classification.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2deJ4OTW5lc",
        "colab_type": "text"
      },
      "source": [
        "### B5. Saving weignts of model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ZMrC7WnW5lc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save_weights('cnn_classification.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frlsMalwW5lc",
        "colab_type": "text"
      },
      "source": [
        "### B6. Loading the Model weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTELY3oUW5lc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.load_weights('cnn_classification.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZm6uHDdW5lc",
        "colab_type": "text"
      },
      "source": [
        "### C. Performance Measures\n",
        "\n",
        "**Now we are going to plot the accuracy and loss **"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3M0DRwxvW5lc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "train_loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "MnYg1NdNW5lc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = range(len(acc)) \n",
        "plt.plot(epochs, train_acc, 'b', label='Training Accuracy')\n",
        "plt.plot(epochs, val_acc, 'r', label='Validation Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "plt.show()\n",
        "\n",
        "plt.plot(epochs, train_loss, 'b', label='Training Loss')\n",
        "plt.plot(epochs, val_loss, 'r', label='Validation Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36Y8FAgIW5lc",
        "colab_type": "text"
      },
      "source": [
        "# Model Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJKAyCIsW5lc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get the filenames from the generator\n",
        "fnames = test_generator.filenames\n",
        " \n",
        "# Get the ground truth from generator\n",
        "ground_truth = test_generator.classes\n",
        " \n",
        "# Get the label to class mapping from the generator\n",
        "label2index = test_generator.class_indices\n",
        " \n",
        "# Getting the mapping from class index to class label\n",
        "idx2label = dict((v,k) for k,v in label2index.items())\n",
        " \n",
        "# Get the predictions from the model using the generator\n",
        "predictions = model.predict_generator(test_generator, steps=test_generator.samples/test_generator.batch_size,verbose=1)\n",
        "predicted_classes = np.argmax(predictions,axis=1)\n",
        " \n",
        "errors = np.where(predicted_classes != ground_truth)[0]\n",
        "print(\"No of errors = {}/{}\".format(len(errors),test_generator.samples))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwBivBfMW5lc",
        "colab_type": "text"
      },
      "source": [
        "# Assignemnt\n",
        "**You have to load the weights of previous model and with the help of previous weights try to create a CNN model with one more convolution layers. You have to train only after the newly added convolution layers of the neural network. **\n",
        "\n",
        "Hint : Use model.load_weights('weights.h5', by_name=True)\n"
      ]
    }
  ]
}